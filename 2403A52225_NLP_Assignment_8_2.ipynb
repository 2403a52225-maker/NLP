{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMSS/4VG8kKvmqg/vLvM7UR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2403a52225-maker/NLP/blob/main/2403A52225_NLP_Assignment_8_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Corpus**"
      ],
      "metadata": {
        "id": "2q7Go5xUYRib"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Em_gaX7iPC_a"
      },
      "outputs": [],
      "source": [
        "D1  = \"I am learning natural language processing concepts\"\n",
        "D2  = \"Natural language processing deals with text and language data\"\n",
        "D3  = \"Tokenization is an important step in NLP\"\n",
        "D4  = \"Stemming and lemmatization are used in text preprocessing\"\n",
        "D5  = \"I apply machine learning algorithms for text classification\"\n",
        "D6  = \"Text data is converted into numerical features using TF IDF\"\n",
        "D7  = \"Word embeddings help represent words in vector form\"\n",
        "D8  = \"NLP models are used for sentiment analysis\"\n",
        "D9  = \"Deep learning techniques improve natural language understanding\"\n",
        "D10 = \"Chatbots and virtual assistants use NLP techniques\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Uni Gram Counts**"
      ],
      "metadata": {
        "id": "FaOzy7m4YMwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "# Combine the text from D1, D2, D3, D4\n",
        "combined_text = f\"{D1} {D2} {D3} {D4}\"\n",
        "\n",
        "# Tokenize the combined text into words and convert to lowercase\n",
        "words = combined_text.lower().split()\n",
        "\n",
        "# Calculate unigram counts\n",
        "unigram_counts = collections.Counter(words)\n",
        "\n",
        "# Print the unigram counts\n",
        "print(\"Unigram Counts:\")\n",
        "for word, count in unigram_counts.most_common():\n",
        "    print(f\"{word}: {count}\")\n",
        "#Vocabulary size is length of unigrams\n",
        "V=len(unigram_counts)\n",
        "print(\"Vocabulary Size=\",V)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNCsW-AvPOes",
        "outputId": "a4ad12bb-daad-47a4-b32e-6780b17976a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigram Counts:\n",
            "language: 3\n",
            "natural: 2\n",
            "processing: 2\n",
            "text: 2\n",
            "and: 2\n",
            "in: 2\n",
            "i: 1\n",
            "am: 1\n",
            "learning: 1\n",
            "concepts: 1\n",
            "deals: 1\n",
            "with: 1\n",
            "data: 1\n",
            "tokenization: 1\n",
            "is: 1\n",
            "an: 1\n",
            "important: 1\n",
            "step: 1\n",
            "nlp: 1\n",
            "stemming: 1\n",
            "lemmatization: 1\n",
            "are: 1\n",
            "used: 1\n",
            "preprocessing: 1\n",
            "Vocabulary Size= 24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bi-Gram Counts**"
      ],
      "metadata": {
        "id": "2glFp0pgYI7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "\n",
        "combined_text = f\"{D1} {D2} {D3} {D4}\"\n",
        "words = combined_text.lower().split()\n",
        "\n",
        "# Generate bigrams\n",
        "bigrams = []\n",
        "for i in range(len(words) - 1):\n",
        "    bigrams.append((words[i], words[i+1]))\n",
        "\n",
        "# Calculate bigram counts\n",
        "bigram_counts = collections.Counter(bigrams)\n",
        "\n",
        "# Print the bigram counts\n",
        "print(\"\\nBigram Counts:\")\n",
        "for bigram, count in bigram_counts.most_common():\n",
        "    print(f\"{bigram[0]} {bigram[1]}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHxPoBJ3PVfI",
        "outputId": "60d4cc8a-74ec-48fe-8127-c9707991e006"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Bigram Counts:\n",
            "natural language: 2\n",
            "language processing: 2\n",
            "i am: 1\n",
            "am learning: 1\n",
            "learning natural: 1\n",
            "processing concepts: 1\n",
            "concepts natural: 1\n",
            "processing deals: 1\n",
            "deals with: 1\n",
            "with text: 1\n",
            "text and: 1\n",
            "and language: 1\n",
            "language data: 1\n",
            "data tokenization: 1\n",
            "tokenization is: 1\n",
            "is an: 1\n",
            "an important: 1\n",
            "important step: 1\n",
            "step in: 1\n",
            "in nlp: 1\n",
            "nlp stemming: 1\n",
            "stemming and: 1\n",
            "and lemmatization: 1\n",
            "lemmatization are: 1\n",
            "are used: 1\n",
            "used in: 1\n",
            "in text: 1\n",
            "text preprocessing: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tri-Gram Counts"
      ],
      "metadata": {
        "id": "s62I7TqNYD6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "\n",
        "combined_text = f\"{D1} {D2} {D3} {D4}\"\n",
        "words = combined_text.lower().split()\n",
        "\n",
        "# Generate bigrams\n",
        "Trigrams = []\n",
        "for i in range(len(words) - 2):\n",
        "    Trigrams.append((words[i], words[i+1], words[i+2]))\n",
        "\n",
        "# Calculate bigram counts\n",
        "Trigrams_counts = collections.Counter(Trigrams)\n",
        "\n",
        "# Print the bigram counts\n",
        "print(\"\\nTrigrams Counts:\")\n",
        "for Trigrams, count in Trigrams_counts.most_common():\n",
        "    print(f\"{Trigrams[0]} {Trigrams[1]} {Trigrams[2]}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpVuVzddPZKb",
        "outputId": "3869adcd-8785-4052-aa6e-dbe93027f321"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trigrams Counts:\n",
            "natural language processing: 2\n",
            "i am learning: 1\n",
            "am learning natural: 1\n",
            "learning natural language: 1\n",
            "language processing concepts: 1\n",
            "processing concepts natural: 1\n",
            "concepts natural language: 1\n",
            "language processing deals: 1\n",
            "processing deals with: 1\n",
            "deals with text: 1\n",
            "with text and: 1\n",
            "text and language: 1\n",
            "and language data: 1\n",
            "language data tokenization: 1\n",
            "data tokenization is: 1\n",
            "tokenization is an: 1\n",
            "is an important: 1\n",
            "an important step: 1\n",
            "important step in: 1\n",
            "step in nlp: 1\n",
            "in nlp stemming: 1\n",
            "nlp stemming and: 1\n",
            "stemming and lemmatization: 1\n",
            "and lemmatization are: 1\n",
            "lemmatization are used: 1\n",
            "are used in: 1\n",
            "used in text: 1\n",
            "in text preprocessing: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Next Word Prediction Using Bi-Gram Counts**"
      ],
      "metadata": {
        "id": "elYX60uhX_mM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word_bigram(word_sequence, bigram_counts, unigram_counts):\n",
        "    # Tokenize the input sequence and get the last word\n",
        "    words_in_sequence = word_sequence.lower().split()\n",
        "    if not words_in_sequence:\n",
        "        return \"Please provide a word sequence.\"\n",
        "    last_word = words_in_sequence[-1]\n",
        "\n",
        "    # Find potential next words based on bigrams starting with last_word\n",
        "    potential_next_words = {}\n",
        "    for (w1, w2), count in bigram_counts.items():\n",
        "        if w1 == last_word:\n",
        "            potential_next_words[w2] = count\n",
        "\n",
        "    if not potential_next_words:\n",
        "        return f\"No bigram found starting with '{last_word}'.\"\n",
        "\n",
        "    # Calculate probabilities for potential next words\n",
        "    # P(w2 | w1) = Count(w1, w2) / Count(w1)\n",
        "    last_word_unigram_count = unigram_counts.get(last_word, 0)\n",
        "    if last_word_unigram_count == 0:\n",
        "        return f\"'{last_word}' not found in unigram counts. Cannot predict next word.\"\n",
        "\n",
        "    predicted_word = None\n",
        "    max_probability = -1\n",
        "\n",
        "    for next_word, bigram_count in potential_next_words.items():\n",
        "        probability = bigram_count / last_word_unigram_count\n",
        "        print(\"probability of  \" f'{next_word}' \" is \", probability)\n",
        "        if probability > max_probability:\n",
        "            max_probability = probability\n",
        "            predicted_word = next_word\n",
        "\n",
        "    return predicted_word\n",
        "\n",
        "# Example usage:\n",
        "# Ensure bigram_counts and unigram_counts are defined from previous cells\n",
        "# (They are present in the kernel state.)\n",
        "\n",
        "# Try with a word sequence\n",
        "sequence1 = \"i am\"\n",
        "next_word1 = predict_next_word_bigram(sequence1, bigram_counts, unigram_counts)\n",
        "print(f\"Given sequence: '{sequence1}', predicted next word: '{next_word1}'\")\n",
        "\n",
        "sequence2 = \"i am learning\"\n",
        "next_word2 = predict_next_word_bigram(sequence2, bigram_counts, unigram_counts)\n",
        "print(f\"Given sequence: '{sequence2}', predicted next word: '{next_word2}'\")\n",
        "\n",
        "sequence3 = \"language processing\"\n",
        "next_word3 = predict_next_word_bigram(sequence3, bigram_counts, unigram_counts)\n",
        "print(f\"Given sequence: '{sequence3}', predicted next word: '{next_word3}'\")\n",
        "\n",
        "sequence4 = \"nonexistent word\"\n",
        "next_word4 = predict_next_word_bigram(sequence4, bigram_counts, unigram_counts)\n",
        "print(f\"Given sequence: '{sequence4}', predicted next word: '{next_word4}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENAc-6YkQq9A",
        "outputId": "1bd770cb-26f5-4079-bd87-b41a99a5a78a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "probability of  learning is  1.0\n",
            "Given sequence: 'i am', predicted next word: 'learning'\n",
            "probability of  natural is  1.0\n",
            "Given sequence: 'i am learning', predicted next word: 'natural'\n",
            "probability of  concepts is  0.5\n",
            "probability of  deals is  0.5\n",
            "Given sequence: 'language processing', predicted next word: 'concepts'\n",
            "Given sequence: 'nonexistent word', predicted next word: 'No bigram found starting with 'word'.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deployment of Bi-Gram Model**"
      ],
      "metadata": {
        "id": "Pk_zxBuLX5vX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ip_text=input(\"enter text\")\n",
        "next_word1 = predict_next_word_bigram(ip_text, bigram_counts, unigram_counts)\n",
        "print(f\"Given sequence: '{ip_text}', predicted next word: '{next_word1}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxO9yFrcQyrp",
        "outputId": "53dcf328-a218-47e4-e014-6fdab9a9991b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter textin nlp\n",
            "probability of  stemming is  1.0\n",
            "Given sequence: 'in nlp', predicted next word: 'stemming'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Next Word Prediction Using Tri-Gram Counts**"
      ],
      "metadata": {
        "id": "ncp8iQFfX1-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word_trigram(word_sequence, Trigrams_counts, bigram_counts):\n",
        "    # Tokenize the input sequence\n",
        "    words_in_sequence = word_sequence.lower().split()\n",
        "    if not words_in_sequence:\n",
        "        return \"Please provide a word sequence.\"\n",
        "\n",
        "    # Ensure at least two words for trigram prediction\n",
        "    if len(words_in_sequence) < 2:\n",
        "        return \"Sequence must contain at least two words for trigram prediction.\"\n",
        "\n",
        "    # Get the last two words as a tuple\n",
        "    last_two_words_tuple = tuple(words_in_sequence[-2:])\n",
        "\n",
        "    # Find potential next words based on trigrams starting with the last two words\n",
        "    potential_next_words = {}\n",
        "    for (w1, w2, w3), count in Trigrams_counts.items():\n",
        "        if (w1, w2) == last_two_words_tuple:\n",
        "            potential_next_words[w3] = count\n",
        "\n",
        "    if not potential_next_words:\n",
        "        return f\"No trigram found starting with '{' '.join(last_two_words_tuple)}'.\"\n",
        "\n",
        "    # Calculate probabilities for potential next words\n",
        "    # P(w3 | w1,w2) = Count(w1, w2, w3) / Count(w1, w2)\n",
        "    # The denominator should be the count of the bigram (w1, w2)\n",
        "    last_two_words_bigram_count = bigram_counts.get(last_two_words_tuple, 0)\n",
        "    if last_two_words_bigram_count == 0:\n",
        "        return f\"'{' '.join(last_two_words_tuple)}' not found as a bigram. Cannot predict next word.\"\n",
        "\n",
        "    predicted_word = None\n",
        "    max_probability = -1\n",
        "\n",
        "    for next_word, trigram_count in potential_next_words.items():\n",
        "        probability = trigram_count / last_two_words_bigram_count\n",
        "        print(\"probability of  \" f'{next_word}' \" is \", probability)\n",
        "        if probability > max_probability:\n",
        "            max_probability = probability\n",
        "            predicted_word = next_word\n",
        "\n",
        "    return predicted_word\n",
        "\n",
        "# Example usage:\n",
        "# Ensure bigram_counts, unigram_counts, and Trigrams_counts are defined from previous cells\n",
        "# (They are present in the kernel state.)\n",
        "\n",
        "# Try with a word sequence\n",
        "sequence1 = \"i am learning\"\n",
        "next_word1 = predict_next_word_trigram(sequence1, Trigrams_counts, bigram_counts)\n",
        "print(f\"Given sequence: '{sequence1}', predicted next word: '{next_word1}'\")\n",
        "\n",
        "sequence2 = \"natural language processing\"\n",
        "next_word2 = predict_next_word_trigram(sequence2, Trigrams_counts, bigram_counts)\n",
        "print(f\"Given sequence: '{sequence2}', predicted next word: '{next_word2}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T19tJUPAQ66U",
        "outputId": "dc497cfa-54ed-4d2a-9616-115a3f3275d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "probability of  natural is  1.0\n",
            "Given sequence: 'i am learning', predicted next word: 'natural'\n",
            "probability of  concepts is  0.5\n",
            "probability of  deals is  0.5\n",
            "Given sequence: 'natural language processing', predicted next word: 'concepts'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deployment of Tri-Gram Model**"
      ],
      "metadata": {
        "id": "TQeqkyj2XvtP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ip_text=input(\"enter text\")\n",
        "next_word1 = predict_next_word_trigram(ip_text, Trigrams_counts, bigram_counts)\n",
        "print(f\"Given sequence: '{ip_text}', predicted next word: '{next_word1}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSN4MMj-Rr7Y",
        "outputId": "4dc9bc00-fb97-4ed9-b809-dce9d2d26748"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter textI am\n",
            "probability of  learning is  1.0\n",
            "Given sequence: 'I am', predicted next word: 'learning'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Next Word Prediction Using Bi-Gram Counts with Laplace Smoothening**"
      ],
      "metadata": {
        "id": "6ng-gOLeXr9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word_bigram_Laplace(word_sequence, bigram_counts, unigram_counts):\n",
        "    # Tokenize the input sequence and get the last word\n",
        "    words_in_sequence = word_sequence.lower().split()\n",
        "    if not words_in_sequence:\n",
        "        return \"Please provide a word sequence.\"\n",
        "    last_word = words_in_sequence[-1]\n",
        "\n",
        "    # Find potential next words based on bigrams starting with last_word\n",
        "    potential_next_words = {}\n",
        "    for (w1, w2), count in bigram_counts.items():\n",
        "        if w1 == last_word:\n",
        "            potential_next_words[w2] = count\n",
        "\n",
        "    if not potential_next_words:\n",
        "        return f\"No bigram found starting with '{last_word}'.\"\n",
        "\n",
        "    # Calculate probabilities for potential next words\n",
        "    # P(w2 | w1) = Count(w1, w2) / Count(w1)\n",
        "    last_word_unigram_count = unigram_counts.get(last_word, 0)\n",
        "    if last_word_unigram_count == 0:\n",
        "        return f\"'{last_word}' not found in unigram counts. Cannot predict next word.\"\n",
        "\n",
        "    predicted_word = None\n",
        "    max_probability = -1\n",
        "\n",
        "    for next_word, bigram_count in potential_next_words.items():\n",
        "        probability = (bigram_count+1) / (last_word_unigram_count+V)\n",
        "        print(\"probability of \" f'{next_word}' \" is \", probability)\n",
        "        if probability > max_probability:\n",
        "            max_probability = probability\n",
        "            predicted_word = next_word\n",
        "\n",
        "    return predicted_word\n",
        "\n",
        "# Example usage:\n",
        "# Ensure bigram_counts and unigram_counts are defined from previous cells\n",
        "# (They are present in the kernel state.)\n",
        "\n",
        "# Try with a word sequence\n",
        "sequence1 = \"I am\"\n",
        "next_word1 = predict_next_word_bigram_Laplace(sequence1, bigram_counts, unigram_counts)\n",
        "print(f\"Given sequence: '{sequence1}', predicted next word: '{next_word1}'\")\n",
        "\n",
        "sequence2 = \"I am learning\"\n",
        "next_word2 = predict_next_word_bigram_Laplace(sequence2, bigram_counts, unigram_counts)\n",
        "print(f\"Given sequence: '{sequence2}', predicted next word: '{next_word2}'\")\n",
        "\n",
        "sequence3 = \"language processing\"\n",
        "next_word3 = predict_next_word_bigram_Laplace(sequence3, bigram_counts, unigram_counts)\n",
        "print(f\"Given sequence: '{sequence3}', predicted next word: '{next_word3}'\")\n",
        "\n",
        "sequence4 = \"nonexistent word\"\n",
        "next_word4 = predict_next_word_bigram_Laplace(sequence4, bigram_counts, unigram_counts)\n",
        "print(f\"Given sequence: '{sequence4}', predicted next word: '{next_word4}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2O1tuNFRzql",
        "outputId": "eb0ba7c2-4212-4498-d37b-078b6a3d9cbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "probability of learning is  0.08\n",
            "Given sequence: 'I am', predicted next word: 'learning'\n",
            "probability of natural is  0.08\n",
            "Given sequence: 'I am learning', predicted next word: 'natural'\n",
            "probability of concepts is  0.07692307692307693\n",
            "probability of deals is  0.07692307692307693\n",
            "Given sequence: 'language processing', predicted next word: 'concepts'\n",
            "Given sequence: 'nonexistent word', predicted next word: 'No bigram found starting with 'word'.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deployment of Laplace Smoothening based Bi-Gram Model**"
      ],
      "metadata": {
        "id": "faZAavBWXncF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ip_text = input(\"Enter text: \")\n",
        "next_word = predict_next_word_bigram_Laplace(ip_text,bigram_counts,unigram_counts)\n",
        "print(f\"\\nGiven sequence: '{ip_text}', predicted next word: '{next_word}'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdmIfIuyWEp9",
        "outputId": "049616b9-f4b4-492f-c022-f7a946934e2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter text: in nlp\n",
            "probability of stemming is  0.08\n",
            "\n",
            "Given sequence: 'in nlp', predicted next word: 'stemming'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Next Word Prediction Using Tri-Gram Counts based on laplace smoothening**"
      ],
      "metadata": {
        "id": "Nx73RCWTXiEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word_trigram_Laplace(word_sequence, Trigrams_counts, bigram_counts):\n",
        "    # Tokenize the input sequence\n",
        "    words_in_sequence = word_sequence.lower().split()\n",
        "    if not words_in_sequence:\n",
        "        return \"Please provide a word sequence.\"\n",
        "\n",
        "    # Ensure at least two words for trigram prediction\n",
        "    if len(words_in_sequence) < 2:\n",
        "        return \"Sequence must contain at least two words for trigram prediction.\"\n",
        "\n",
        "    # Get the last two words as a tuple\n",
        "    last_two_words_tuple = tuple(words_in_sequence[-2:])\n",
        "\n",
        "    # Find potential next words based on trigrams starting with the last two words\n",
        "    potential_next_words = {}\n",
        "    for (w1, w2, w3), count in Trigrams_counts.items():\n",
        "        if (w1, w2) == last_two_words_tuple:\n",
        "            potential_next_words[w3] = count\n",
        "\n",
        "    if not potential_next_words:\n",
        "        return f\"No trigram found starting with '{' '.join(last_two_words_tuple)}'.\"\n",
        "\n",
        "    # Calculate probabilities for potential next words\n",
        "    # P(w3 | w1,w2) = Count(w1, w2, w3) / Count(w1, w2)\n",
        "    # The denominator should be the count of the bigram (w1, w2)\n",
        "    last_two_words_bigram_count = bigram_counts.get(last_two_words_tuple, 0)\n",
        "    if last_two_words_bigram_count == 0:\n",
        "        return f\"'{' '.join(last_two_words_tuple)}' not found as a bigram. Cannot predict next word.\"\n",
        "\n",
        "    predicted_word = None\n",
        "    max_probability = -1\n",
        "\n",
        "    for next_word, trigram_count in potential_next_words.items():\n",
        "        probability = (trigram_count+1) / (last_two_words_bigram_count+V)\n",
        "        print(\"probability of  \" f'{next_word}' \" is \", probability)\n",
        "        if probability > max_probability:\n",
        "            max_probability = probability\n",
        "            predicted_word = next_word\n",
        "\n",
        "    return predicted_word\n",
        "\n",
        "# Example usage:\n",
        "# Ensure bigram_counts, unigram_counts, and Trigrams_counts are defined from previous cells\n",
        "# (They are present in the kernel state.)\n",
        "\n",
        "# Try with a word sequence\n",
        "sequence1 = \"I am learning\"\n",
        "next_word1 = predict_next_word_trigram_Laplace(sequence1, Trigrams_counts, bigram_counts)\n",
        "print(f\"Given sequence: '{sequence1}', predicted next word: '{next_word1}'\")\n",
        "\n",
        "sequence2 = \"natural language processing\"\n",
        "next_word2 = predict_next_word_trigram_Laplace(sequence2, Trigrams_counts, bigram_counts)\n",
        "print(f\"Given sequence: '{sequence2}', predicted next word: '{next_word2}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhej-5J8WNsn",
        "outputId": "14680e66-83a5-4fac-f371-4b4a476b8a53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "probability of  natural is  0.08\n",
            "Given sequence: 'I am learning', predicted next word: 'natural'\n",
            "probability of  concepts is  0.07692307692307693\n",
            "probability of  deals is  0.07692307692307693\n",
            "Given sequence: 'natural language processing', predicted next word: 'concepts'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deployment of Laplace Smoothening based Tri-Gram Model**"
      ],
      "metadata": {
        "id": "V5vTzFDVXc9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ip_text=input(\"enter text\")\n",
        "next_word1 = predict_next_word_trigram_Laplace(ip_text, Trigrams_counts, bigram_counts)\n",
        "print(f\"Given sequence: '{ip_text}', predicted next word: '{next_word1}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WD9LiyzMWVDT",
        "outputId": "557732c1-9beb-41e7-e15d-edb98674871b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter texti am learning\n",
            "probability of  natural is  0.08\n",
            "Given sequence: 'i am learning', predicted next word: 'natural'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Next Word Prediction Using Bi-Gram Counts with Add - K Smoothening**"
      ],
      "metadata": {
        "id": "H4XlpDWDXXXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word_bigram_K(word_sequence, bigram_counts, unigram_counts, K): #K=0.5-0.01\n",
        "    # Tokenize the input sequence and get the last word\n",
        "    words_in_sequence = word_sequence.lower().split()\n",
        "    if not words_in_sequence:\n",
        "        return \"Please provide a word sequence.\"\n",
        "    last_word = words_in_sequence[-1]\n",
        "\n",
        "    # Find potential next words based on bigrams starting with last_word\n",
        "    potential_next_words = {}\n",
        "    for (w1, w2), count in bigram_counts.items():\n",
        "        if w1 == last_word:\n",
        "            potential_next_words[w2] = count\n",
        "\n",
        "    if not potential_next_words:\n",
        "        return f\"No bigram found starting with '{last_word}'.\"\n",
        "\n",
        "    # Calculate probabilities for potential next words\n",
        "    # P(w2 | w1) = Count(w1, w2) / Count(w1)\n",
        "    last_word_unigram_count = unigram_counts.get(last_word, 0)\n",
        "    if last_word_unigram_count == 0:\n",
        "        return f\"'{last_word}' not found in unigram counts. Cannot predict next word.\"\n",
        "\n",
        "    predicted_word = None\n",
        "    max_probability = -1\n",
        "\n",
        "    for next_word, bigram_count in potential_next_words.items():\n",
        "        probability = (bigram_count+K) / (last_word_unigram_count+K*V)\n",
        "        print(\"probability of \" f'{next_word}' \" is \", probability)\n",
        "        if probability > max_probability:\n",
        "            max_probability = probability\n",
        "            predicted_word = next_word\n",
        "\n",
        "    return predicted_word\n",
        "\n",
        "# Example usage:\n",
        "# Ensure bigram_counts and unigram_counts are defined from previous cells\n",
        "# (They are present in the kernel state.)\n",
        "\n",
        "# Try with a word sequence\n",
        "sequence1 = \"I am\"\n",
        "next_word1 = predict_next_word_bigram_K(sequence1, bigram_counts, unigram_counts,0.5)\n",
        "print(f\"Given sequence: '{sequence1}', predicted next word: '{next_word1}'\")\n",
        "\n",
        "sequence2 = \"I am learning\"\n",
        "next_word2 = predict_next_word_bigram_K(sequence2, bigram_counts, unigram_counts,0.5)\n",
        "print(f\"Given sequence: '{sequence2}', predicted next word: '{next_word2}'\")\n",
        "\n",
        "sequence3 = \"language processing\"\n",
        "next_word3 = predict_next_word_bigram_K(sequence3, bigram_counts, unigram_counts,0.5)\n",
        "print(f\"Given sequence: '{sequence3}', predicted next word: '{next_word3}'\")\n",
        "\n",
        "sequence4 = \"nonexistent word\"\n",
        "next_word4 = predict_next_word_bigram_K(sequence4, bigram_counts, unigram_counts,0.5)\n",
        "print(f\"Given sequence: '{sequence4}', predicted next word: '{next_word4}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sn-gp7jEWckm",
        "outputId": "ea13ef75-6f37-4c85-bef8-5242d46d564c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "probability of learning is  0.11538461538461539\n",
            "Given sequence: 'I am', predicted next word: 'learning'\n",
            "probability of natural is  0.11538461538461539\n",
            "Given sequence: 'I am learning', predicted next word: 'natural'\n",
            "probability of concepts is  0.10714285714285714\n",
            "probability of deals is  0.10714285714285714\n",
            "Given sequence: 'language processing', predicted next word: 'concepts'\n",
            "Given sequence: 'nonexistent word', predicted next word: 'No bigram found starting with 'word'.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deployment of Add-K Smoothening based Bi-Gram Model**"
      ],
      "metadata": {
        "id": "cTjNevIcXTEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ip_text=input(\"enter text\")\n",
        "next_word1 = predict_next_word_bigram_K(ip_text, bigram_counts, unigram_counts,0.5)\n",
        "print(f\"Given sequence: '{ip_text}', predicted next word: '{next_word1}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3D33n536WqJk",
        "outputId": "1605d896-4464-4faa-a0b7-a7cb48b405da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter texti am\n",
            "probability of learning is  0.11538461538461539\n",
            "Given sequence: 'i am', predicted next word: 'learning'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Next Word Prediction Using Tri-Gram Counts with Add - K Smoothening**"
      ],
      "metadata": {
        "id": "5pjoK6baXPr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word_trigram_K(word_sequence, Trigrams_counts, bigram_counts,K): #K=0.5-0.01\n",
        "    # Tokenize the input sequence\n",
        "    words_in_sequence = word_sequence.lower().split()\n",
        "    if not words_in_sequence:\n",
        "        return \"Please provide a word sequence.\"\n",
        "\n",
        "    # Ensure at least two words for trigram prediction\n",
        "    if len(words_in_sequence) < 2:\n",
        "        return \"Sequence must contain at least two words for trigram prediction.\"\n",
        "\n",
        "    # Get the last two words as a tuple\n",
        "    last_two_words_tuple = tuple(words_in_sequence[-2:])\n",
        "\n",
        "    # Find potential next words based on trigrams starting with the last two words\n",
        "    potential_next_words = {}\n",
        "    for (w1, w2, w3), count in Trigrams_counts.items():\n",
        "        if (w1, w2) == last_two_words_tuple:\n",
        "            potential_next_words[w3] = count\n",
        "\n",
        "    if not potential_next_words:\n",
        "        return f\"No trigram found starting with '{' '.join(last_two_words_tuple)}'.\"\n",
        "\n",
        "    # Calculate probabilities for potential next words\n",
        "    # P(w3 | w1,w2) = Count(w1, w2, w3) / Count(w1, w2)\n",
        "    # The denominator should be the count of the bigram (w1, w2)\n",
        "    last_two_words_bigram_count = bigram_counts.get(last_two_words_tuple, 0)\n",
        "    if last_two_words_bigram_count == 0:\n",
        "        return f\"'{' '.join(last_two_words_tuple)}' not found as a bigram. Cannot predict next word.\"\n",
        "\n",
        "    predicted_word = None\n",
        "    max_probability = -1\n",
        "\n",
        "    for next_word, trigram_count in potential_next_words.items():\n",
        "        probability = (trigram_count+K) / (last_two_words_bigram_count+K*V)\n",
        "        print(\"probability of  \" f'{next_word}' \" is \", probability)\n",
        "        if probability > max_probability:\n",
        "            max_probability = probability\n",
        "            predicted_word = next_word\n",
        "\n",
        "    return predicted_word\n",
        "\n",
        "# Example usage:\n",
        "# Ensure bigram_counts, unigram_counts, and Trigrams_counts are defined from previous cells\n",
        "# (They are present in the kernel state.)\n",
        "\n",
        "# Try with a word sequence\n",
        "sequence1 = \"I am learning\"\n",
        "next_word1 = predict_next_word_trigram_K(sequence1, Trigrams_counts, bigram_counts,0.5)\n",
        "print(f\"Given sequence: '{sequence1}', predicted next word: '{next_word1}'\")\n",
        "\n",
        "sequence2 = \"natural language processing\"\n",
        "next_word2 = predict_next_word_trigram_K(sequence2, Trigrams_counts, bigram_counts,0.5)\n",
        "print(f\"Given sequence: '{sequence2}', predicted next word: '{next_word2}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoDvMabuWxmZ",
        "outputId": "b9240089-1bdc-4b5f-c0f4-cfbf75b6f87a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "probability of  natural is  0.11538461538461539\n",
            "Given sequence: 'I am learning', predicted next word: 'natural'\n",
            "probability of  concepts is  0.10714285714285714\n",
            "probability of  deals is  0.10714285714285714\n",
            "Given sequence: 'natural language processing', predicted next word: 'concepts'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deployment of Add-K Smoothening based Tri-Gram Model**"
      ],
      "metadata": {
        "id": "S33tziXiXKgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ip_text=input(\"enter text\")\n",
        "next_word1 = predict_next_word_trigram_K(ip_text, Trigrams_counts, bigram_counts,0.5)\n",
        "print(f\"Given sequence: '{ip_text}', predicted next word: '{next_word1}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FiVFVlpW2CQ",
        "outputId": "8d857471-2e29-4d52-9ac2-380bf43db757"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter texti am learning\n",
            "probability of  natural is  0.11538461538461539\n",
            "Given sequence: 'i am learning', predicted next word: 'natural'\n"
          ]
        }
      ]
    }
  ]
}